{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOtyIBVraX79or3p3Gei2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tri334/Naive_Hoax_Covid/blob/master/filter_hoax_naive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1UYVc_VmX7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install Sastrawi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbczwkqvlJOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as sp\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "import base64\n",
        "import requests\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "corpus_url = 'https://raw.githubusercontent.com/Tri334/Naive_Hoax_Covid/master/corpus.txt'\n",
        "stop_url = 'https://raw.githubusercontent.com/Tri334/Naive_Hoax_Covid/master/stopword2016.txt'\n",
        "\n",
        "req = requests.get(stop_url)\n",
        "req = req.text\n",
        "stopword = req.splitlines()\n",
        "\n",
        "corpus = requests.get(corpus_url)\n",
        "corpus = corpus.text\n",
        "\n",
        "def openFile(corpus):\n",
        "    file = open(corpus, encoding=\"utf8\")\n",
        "    soup = sp(file,'html.parser')\n",
        "    doc_berita = soup.find_all(\"doc\")\n",
        "    return doc_berita\n",
        "\n",
        "def priorProbability(berita):\n",
        "    categori_only = {}\n",
        "    prior_proba ={}\n",
        "    jumlah = 0\n",
        "    for item in berita:\n",
        "        cat = item.find('cat').text\n",
        "        cat = preprocessing(cat)\n",
        "        if cat:\n",
        "            if cat in categori_only:\n",
        "                categori_only[cat]+=1\n",
        "            else:\n",
        "                categori_only[cat]=1\n",
        "    for item in categori_only:\n",
        "        jumlah+=categori_only[item]\n",
        "    for items in categori_only:\n",
        "        prior_proba[items]=(categori_only[items]/jumlah)\n",
        "    return prior_proba\n",
        "\n",
        "def preprocessing(berita):\n",
        "    cleaning_words = re.sub(\"\\W+\",\" \", berita)\n",
        "    clean = cleaning_words.lower()\n",
        "\n",
        "    tokenize = re.findall(\"\\w+\", clean)\n",
        "    token = \" \".join(tokenize)\n",
        "    return stemmer.stem(token)\n",
        "\n",
        "def webCraw(berita):\n",
        "    list_berita = []\n",
        "    list_berita_cat = []\n",
        "    list_tidak_ada_cat = []\n",
        "    for item in berita:\n",
        "        berita = item.find(\"berita\").text\n",
        "        cat = item.find(\"cat\").text\n",
        "        if cat:\n",
        "            cat = preprocessing(cat)\n",
        "            berita = preprocessing(berita)\n",
        "            list_berita.append(berita)\n",
        "            list_berita_cat.append([berita] + [cat])\n",
        "        else:\n",
        "            list_tidak_ada_cat.append(berita)\n",
        "    with open(\"list_berita.json\", \"w\") as write_file:\n",
        "        json.dump(list_berita, write_file)\n",
        "\n",
        "    with open(\"list_berita_cat.json\", \"w\") as write_file:\n",
        "        json.dump(list_berita_cat, write_file)\n",
        "\n",
        "    with open(\"list_tidak_ada_cat.json\", \"w\") as write_file:\n",
        "        json.dump(list_tidak_ada_cat, write_file)\n",
        "\n",
        "def termUnik(list_berita,stopword):\n",
        "    unique = []\n",
        "    kata = \" \".join(list_berita)\n",
        "    token = kata.split(' ')\n",
        "    for item in token:\n",
        "        if item:\n",
        "            if item not in stopword:\n",
        "                if item not in unique:\n",
        "                    unique.append(item)\n",
        "    unique.sort()\n",
        "    return unique\n",
        "\n",
        "def dikategorikan(berita_cat):\n",
        "    categorized = {}\n",
        "    for item in berita_cat:\n",
        "        if item[0] or item[1]:\n",
        "            if item[1] in categorized:\n",
        "                old = categorized[item[1]]\n",
        "                categorized.update({item[1]:old +\" \"+item[0]})\n",
        "            else:\n",
        "                categorized[item[1]] = item[0]\n",
        "\n",
        "    for key in categorized:\n",
        "        tokenize = categorized[key].split(' ')\n",
        "        categorized[key]= tokenize\n",
        "\n",
        "    return categorized\n",
        "\n",
        "def termWeight(categori,kata_unik):\n",
        "    weight_cat_dict ={}\n",
        "    for key in categori:\n",
        "        # print(key)\n",
        "        waight_temp = []\n",
        "        tes = {}\n",
        "        for i in range(len(kata_unik)):\n",
        "            score = 0\n",
        "            for item in categori[key]:\n",
        "                if kata_unik[i] == item:\n",
        "                    score += 1\n",
        "            tes[kata_unik[i]] = score\n",
        "            waight_temp.append(score)\n",
        "        weight_cat_dict[key] = tes\n",
        "    return weight_cat_dict\n",
        "\n",
        "def conProba(weight_cat_dict,term_unik):\n",
        "    sum_weight = {}\n",
        "    possible = {}\n",
        "    for item in weight_cat_dict:\n",
        "        term_count = 0\n",
        "        for val in weight_cat_dict[item]:\n",
        "            term_count+=weight_cat_dict[item][val]\n",
        "        sum_weight[item]= term_count\n",
        "\n",
        "    for key in weight_cat_dict:\n",
        "        poss_term=0\n",
        "        temp = {}\n",
        "        for value in weight_cat_dict[key]:\n",
        "            poss_term = weight_cat_dict[key][value]\n",
        "            p_kata = (poss_term+1)/(sum_weight[key]+ len(term_unik))\n",
        "            temp[value]=p_kata\n",
        "        possible[key]=temp\n",
        "    return possible\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzSQdeKqPQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_berita = 'https://raw.githubusercontent.com/Tri334/Naive_Hoax_Covid/master/list_berita.json'\n",
        "list_berita_cat = 'https://raw.githubusercontent.com/Tri334/Naive_Hoax_Covid/master/list_berita_cat.json'\n",
        "\n",
        "list_berita = requests.get(list_berita)\n",
        "list_berita = list_berita.text\n",
        "\n",
        "list_berita_cat = requests.get(list_berita_cat)\n",
        "list_berita_cat = list_berita_cat.text\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq2v6cehsMll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}